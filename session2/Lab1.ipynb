{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c22c465",
   "metadata": {},
   "source": [
    "# Nowcasting Coup Events\n",
    "\n",
    "- Using news headlines from **January 2017 to December 2024**\n",
    "- Focus on **four countries**: Belarus, Bolivia, DR Congo, and Mali\n",
    "- Objective: **train a coup event detector** using a **support set labeled with GPT-generated synthetic annotations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287be414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_corpus = pd.read_csv(\"input/df_corpus.csv\")\n",
    "df_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2970e060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support set\n",
    "df_support = pd.read_csv('input/support.csv')\n",
    "df_support.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ceadf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FewShotX import Embeddings\n",
    "\n",
    "embedder = Embeddings(model_name='all-MiniLM-L6-v2')\n",
    "df_support_embed = embedder.embed_df(df_support, text_col='title')\n",
    "df_support_embed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fe4c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FewShotX import Embeddings\n",
    "\n",
    "run_embeddings_query = False\n",
    "\n",
    "if run_embeddings_query:\n",
    "    # Query set\n",
    "    embedder = Embeddings(model_name='all-MiniLM-L6-v2')\n",
    "    df_query_embed = embedder.embed_df(df_corpus, text_col='title')\n",
    "else:\n",
    "    # Load the precomputed embeddings\n",
    "    df_query_embed = pd.read_csv('temp/df_embed.csv')\n",
    "\n",
    "df_query_embed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60079e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "class ProtoScorer:\n",
    "    def __init__(self, prototype_df, dtype=np.float32, similarity='cosine'):\n",
    "        self.dtype = dtype\n",
    "        self.similarity_type = similarity\n",
    "        \n",
    "        # Extract prototype embeddings\n",
    "        self.prototype_embeddings = prototype_df[[col for col in prototype_df.columns if 'emb' in col]].values.astype(dtype)\n",
    "        \n",
    "        if similarity == 'cosine':\n",
    "            self.prototype_embeddings = normalize(self.prototype_embeddings, axis=1)\n",
    "            self.similarity_fn = self._cosine_similarity\n",
    "        elif similarity == 'dot':\n",
    "            self.similarity_fn = self._dot_product\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported similarity. Use 'cosine' or 'dot'.\")\n",
    "\n",
    "    def _cosine_similarity(self, queries, prototypes):\n",
    "        queries = normalize(queries, axis=1)\n",
    "        sim = np.dot(queries, prototypes.T)\n",
    "        return (sim + 1) / 2  # map cosine similarity from [-1, 1] to [0, 1]\n",
    "\n",
    "    def _dot_product(self, queries, prototypes):\n",
    "        sim = np.dot(queries, prototypes.T)\n",
    "        # Optional normalization for dot product\n",
    "        sim_min, sim_max = sim.min(), sim.max()\n",
    "        return (sim - sim_min) / (sim_max - sim_min + 1e-8)  # scale to [0, 1]\n",
    "\n",
    "    def score(self, query_embeddings):\n",
    "        return np.array(self.similarity_fn(query_embeddings, self.prototype_embeddings), dtype=self.dtype)\n",
    "\n",
    "    def score_dataframe(self, df):\n",
    "        # Extract query embeddings\n",
    "        query_embeddings = df[[col for col in df.columns if 'emb' in col]].values.astype(self.dtype)\n",
    "\n",
    "        # Compute similarity scores\n",
    "        scores = self.score(query_embeddings)\n",
    "        mean_score = np.mean(scores, axis=1)\n",
    "\n",
    "        # Create a DataFrame of similarity scores\n",
    "        score_columns = [f'score_{i}' for i in range(self.prototype_embeddings.shape[0])]\n",
    "        scores_df = pd.DataFrame(scores, columns=score_columns, dtype=self.dtype)\n",
    "\n",
    "        # Merge with metadata and add mean score\n",
    "        non_emb_cols = [col for col in df.columns if 'emb' not in col]\n",
    "        export_df = pd.concat([df[non_emb_cols].reset_index(drop=True), scores_df], axis=1)\n",
    "        export_df['mean_score'] = mean_score\n",
    "\n",
    "        return export_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176d421b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score your large query set\n",
    "scorer = ProtoScorer(df_support_embed, similarity='dot')\n",
    "df_scored = scorer.score_dataframe(df_query_embed)\n",
    "df_scored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab5eaf9",
   "metadata": {},
   "source": [
    "## Unsupervised method: DBSCAN\n",
    "\n",
    "We will cluster articles using `days_since_start`, and `mean_score` (or their scaled versions), this means:\n",
    "+ Articles must be close in time and score\n",
    "+ To form a cluster, there must be at least `k` articles that are close to each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f3f4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the scored DataFrame\n",
    "df_scored = pd.read_csv('temp/df_scored.csv')\n",
    "df_clusters = df_scored[['isocode', 'date', 'title', 'mean_score']].copy()\n",
    "\n",
    "# Just keep the top 10% of the scores\n",
    "df_clusters = df_clusters[df_clusters['mean_score'] >= 0.5]\n",
    "df_clusters = df_clusters.sort_values(by=['isocode', 'date'])\n",
    "df_clusters = df_clusters.reset_index(drop=True)\n",
    "df_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66d7585",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Step 1: Prepare data\n",
    "df = df_clusters.copy()\n",
    "df['date_dt'] = pd.to_datetime(df['date'].astype(str))\n",
    "df['days_since_start'] = (df['date_dt'] - df['date_dt'].min()).dt.days\n",
    "\n",
    "# Step 3: Run DBSCAN by isocode\n",
    "results = []\n",
    "\n",
    "for isocode, group in df.groupby('isocode'):\n",
    "    if len(group) < 2:\n",
    "        continue\n",
    "\n",
    "    group = group.copy()\n",
    "\n",
    "    # Feature matrix: [time, score]\n",
    "    features = group[['days_since_start', 'mean_score']].values\n",
    "    features_scaled = StandardScaler().fit_transform(features)\n",
    "\n",
    "    # DBSCAN clustering\n",
    "    db = DBSCAN(eps=0.5, min_samples=2)\n",
    "    group['cluster'] = db.fit_predict(features_scaled)\n",
    "\n",
    "    results.append(group)\n",
    "\n",
    "# Step 4: Combine results\n",
    "df_clustered_final = pd.concat(results, ignore_index=True)\n",
    "df_clustered_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47b107d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Loop through each country\n",
    "for isocode, group in df_clustered_final.groupby('isocode'):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot unclustered points in grey\n",
    "    noise = group[group['cluster'] == -1]\n",
    "    plt.scatter(noise['date_dt'], noise['mean_score'], color='grey', label='Noise', alpha=0.6)\n",
    "\n",
    "    # Plot each cluster with a different color\n",
    "    clusters = group[group['cluster'] != -1]\n",
    "    for cluster_id in clusters['cluster'].unique():\n",
    "        subcluster = clusters[clusters['cluster'] == cluster_id]\n",
    "        plt.scatter(subcluster['date_dt'], subcluster['mean_score'], label=f'Cluster {cluster_id}', alpha=0.8)\n",
    "\n",
    "    plt.title(f\"{isocode} - mean score over time\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Mean score\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d910e535",
   "metadata": {},
   "source": [
    "## Supervised method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08af282b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Convert date column to month\n",
    "df = pd.read_csv('temp/df_scored.csv')\n",
    "df['date'] = pd.to_datetime(df['date'].astype(str))\n",
    "df['month'] = df['date'].dt.to_period('M').astype(str)  \n",
    "\n",
    "# Define custom top-k functions\n",
    "def topk_mean(k):\n",
    "    return lambda x: np.mean(np.sort(x)[-k:]) if len(x) >= k else np.nan\n",
    "\n",
    "# Aggregation\n",
    "agg_funcs = {\n",
    "    'avg_score': ('mean_score', 'mean'),\n",
    "    'top1_score': ('mean_score', 'max'),\n",
    "    'top3_avg_score': ('mean_score', topk_mean(3)),\n",
    "    'top5_avg_score': ('mean_score', topk_mean(5)),\n",
    "    'top20_avg_score': ('mean_score', topk_mean(20)),\n",
    "    'min': ('mean_score', 'min'),\n",
    "    'perc_75': ('mean_score', lambda x: np.percentile(x, 75)),\n",
    "    'perc_50': ('mean_score', lambda x: np.percentile(x, 50)),\n",
    "    'perc_25': ('mean_score', lambda x: np.percentile(x, 25)),\n",
    "    'std': ('mean_score', 'std'),\n",
    "}\n",
    "\n",
    "# Step 5: Group and aggregate\n",
    "df_monthly = df.groupby(['isocode', 'month']).agg(**agg_funcs).reset_index()\n",
    "df_monthly['period'] = df_monthly['month'].str.replace('-', '').astype(int)\n",
    "df_monthly.drop(columns=['month'], inplace=True)\n",
    "df_monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f3422f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target = pd.read_csv('input/df_target.csv')\n",
    "isocodes_to_keep = df_monthly['isocode'].unique().tolist()\n",
    "df_target = df_target[df_target['isocode'].isin(isocodes_to_keep)]\n",
    "df_target = df_target[(df_target['period'] >= 201701) & (df_target['period'] <= 202412)]\n",
    "df_target.reset_index(drop=True, inplace=True)\n",
    "df_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d45397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two DataFrames\n",
    "df_merged = pd.merge(df_target, df_monthly, on=['isocode', 'period'], how='left')\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50267ddf",
   "metadata": {},
   "source": [
    "## Training a classifier using CatBoost\n",
    "\n",
    "+ Handles NaNs natively (CatBoost automatically learns optimal splits for missing values)\n",
    "+ Allow validation set and early stop to prevent overfitting\n",
    "+ Requires minimal data cleaning — it handles numerical, categorical, and missing data efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6c5b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Split into train and test sets based on time\n",
    "train_df = df_merged[df_merged['period'] < 202401].copy()\n",
    "test_df = df_merged[df_merged['period'] >= 202401].copy()\n",
    "\n",
    "# Feature and target columns\n",
    "feature_cols = [col for col in df_merged.columns if col not in ['isocode', 'period', 'coup']]\n",
    "X = train_df[feature_cols]\n",
    "y = train_df['coup']\n",
    "\n",
    "X_test = test_df[feature_cols]\n",
    "y_test = test_df['coup']\n",
    "\n",
    "# Initialize and train CatBoost model\n",
    "model = CatBoostClassifier(\n",
    "    iterations=300,\n",
    "    learning_rate=0.01,\n",
    "    depth=3,\n",
    "    random_seed=42,\n",
    "    verbose=1)\n",
    "\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbddb31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# Predicted probabilities \n",
    "test_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute F1 score for each threshold\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, test_proba)\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "\n",
    "# Find threshold that gives max F1\n",
    "best_idx = np.argmax(f1_scores)\n",
    "best_threshold = thresholds[best_idx]\n",
    "best_f1 = f1_scores[best_idx]\n",
    "\n",
    "print(f\"Best F1 threshold: {best_threshold:.3f}\")\n",
    "print(f\"Best F1 score: {best_f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c77469",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Use optimal threshold to make final predictions\n",
    "test_preds = (test_proba >= best_threshold).astype(int)\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nClassification Report with Optimal Threshold:\")\n",
    "print(classification_report(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02cb91b",
   "metadata": {},
   "source": [
    "+ `Recall` (0.67) on class 1.0 shows you’re catching 2/3 coups.\n",
    "+ `Precision` is 1.0, meaning no false alarms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05334835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Create results DataFrame\n",
    "results_df = test_df[['isocode', 'period']].copy()\n",
    "results_df['actual_coup'] = y_test.values\n",
    "results_df['predicted_coup'] = test_preds\n",
    "results_df['predicted_proba'] = test_proba\n",
    "\n",
    "print(\"\\nPredicted Coup Events:\")\n",
    "results_df[results_df['actual_coup'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1175bb8",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7831322c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "importances = model.get_feature_importance()\n",
    "feature_names = X.columns\n",
    "\n",
    "# Combine into a DataFrame\n",
    "feat_imp_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': importances\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.barh(feat_imp_df['feature'], feat_imp_df['importance'])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.title(f\"Feature Importances (CatBoost)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
