{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2928379",
   "metadata": {},
   "source": [
    "# Zero-Shot Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a50b9c7",
   "metadata": {},
   "source": [
    "## 1) Generic Zero-Shot Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681f16c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Using a lightweight, fast model for generating embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3125a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input text we want to classify\n",
    "text = \"Opposition party gains ground ahead of national election.\"\n",
    "\n",
    "# Candidate labels representing potential categories\n",
    "labels = [\"economics\", \"sports\", \"politics\"]\n",
    "\n",
    "# --- Generating Embeddings ---\n",
    "# Encode the input text and labels to obtain dense vector representations\n",
    "embed_text = model.encode([text])  # Shape: (1, 384)\n",
    "embed_labels = model.encode(labels)  # Shape: (3, 384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ac94d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarity between the text and each label\n",
    "# Cosine similarity ranges from -1 to 1, so we rescale it to [0, 1] for interpretability\n",
    "sim_matrix = cosine_similarity(embed_text, embed_labels)\n",
    "sim_matrix = (sim_matrix + 1) / 2 \n",
    "print(\"Similarity Scores:\", sim_matrix[0])\n",
    "\n",
    "plt.figure(figsize=(6, 2))\n",
    "plt.barh(labels, sim_matrix.flatten(), color='skyblue')\n",
    "plt.xlabel(\"Probability\")\n",
    "plt.title(\"Generic Zero-Shot - Similarity Scores\")\n",
    "plt.xlim(0, 1)  # Ensure the x-axis is within [0, 1] for clarity\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(alpha=0.3, linestyle='--', axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba5c3bb",
   "metadata": {},
   "source": [
    "## 2) Zero-Shot classifier using NLI\n",
    "\n",
    "+ It uses a model like BART or RoBERTa trained on MNLI to determine how likely a piece of text (premise) entails a hypothesis (constructed from each label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e69564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def ZeroShotNLI(tokenizer, model, text, candidate_labels, multi_label=False, plot=False):\n",
    "    \"\"\"\n",
    "    Perform zero-shot classification using a Natural Language Inference (NLI) model.\n",
    "\n",
    "    This function reformulates each candidate label as a hypothesis and computes the likelihood \n",
    "    that the input text (premise) entails each hypothesis. It supports both single-label \n",
    "    (multi-class) and multi-label classification modes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tokenizer : transformers.PreTrainedTokenizer\n",
    "        The tokenizer associated with the pre-trained NLI model.\n",
    "    model : transformers.PreTrainedModel\n",
    "        The NLI model used to predict entailment probabilities.\n",
    "    text : str\n",
    "        The input text to classify (used as the premise in the NLI formulation).\n",
    "    candidate_labels : List[str]\n",
    "        A list of labels, each of which will be turned into a natural language hypothesis.\n",
    "    multi_label : bool, optional (default=False)\n",
    "        - If False, assumes only one label is true (mutually exclusive labels).\n",
    "        - If True, multiple labels can be simultaneously \"true\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary with:\n",
    "            - \"sequence\": The input text.\n",
    "            - \"labels\": The original list of candidate labels.\n",
    "            - \"scores\": Probabilities corresponding to each label.\n",
    "    \"\"\"\n",
    "    # Template to transform each label into a natural language hypothesis\n",
    "    hypothesis_template = \"This text is about {}.\"\n",
    "    hypotheses = [hypothesis_template.format(label) for label in candidate_labels]\n",
    "\n",
    "    # Identify the position of \"entailment\" in the model's output logits\n",
    "    entailment_id = model.config.label2id.get(\"entailment\", 2)\n",
    "\n",
    "    # Tokenize the (premise, hypothesis) pairs\n",
    "    inputs = tokenizer(\n",
    "        [text] * len(hypotheses), \n",
    "        hypotheses, \n",
    "        return_tensors='pt', \n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    # Run the model\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "    # Multi-label or single-label logic\n",
    "    if multi_label or len(candidate_labels) == 1:\n",
    "        # Apply softmax over [contradiction, entailment] for each label independently\n",
    "        entail_contr_logits = logits[:, [0, entailment_id]]\n",
    "        probs = torch.softmax(entail_contr_logits, dim=1)\n",
    "        entail_probs = probs[:, 1]\n",
    "    else:\n",
    "        # Apply softmax over all labels (single-label setting)\n",
    "        entail_logits = logits[:, entailment_id]\n",
    "        entail_probs = torch.softmax(entail_logits, dim=0)\n",
    "\n",
    "    # Convert to Python list for easy interpretation\n",
    "    entail_probs = entail_probs.tolist()\n",
    "\n",
    "    # Plot the probabilities\n",
    "    if plot:\n",
    "        plt.figure(figsize=(6, 2))\n",
    "        plt.barh(candidate_labels, entail_probs, color='skyblue')\n",
    "        plt.xlabel(\"Probability\")\n",
    "        plt.title(f\"Zero-Shot NLI Classification\")\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.grid(alpha=0.3, linestyle='--')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return {\n",
    "        \"sequence\": text,\n",
    "        \"labels\": candidate_labels,\n",
    "        \"scores\": entail_probs\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9b65f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_name='facebook/bart-large-mnli' # 'roberta-large-mnli'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e24658",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Opposition party gains ground ahead of national election.\"\n",
    "labels = [\"economics\", \"sports\", \"politics\"]\n",
    "\n",
    "ZeroShotNLI(tokenizer, model, text, labels, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b0c2a0",
   "metadata": {},
   "source": [
    "### Multi_label\n",
    "- If `multi_label=False`, assumes only one label is true (mutually exclusive labels).\n",
    "- If `multi_label=True`, multiple labels can be simultaneously \"true\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262111fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ZeroShotNLI(tokenizer, model, text, labels, multi_label=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
